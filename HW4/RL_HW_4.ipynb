{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #4\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this homework, you will have the complete reinforcement-learning experience:  training an agent from scratch to solve a simple domain using Q-learning.\n",
    "\n",
    "The environment you will be applying Q-learning to is called [Taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) (Taxi-v3).  The Taxi problem was introduced by [Dietterich 1998](https://www.jair.org/index.php/jair/article/download/10266/24463) and has been used for reinforcement-learning research in the past.  It is a grid-based environment where the goal of the agent is to pick up a passenger at one location and drop them off at another.\n",
    "\n",
    "The map is fixed and the environment has deterministic transitions.  However, the distinct pickup and drop-off points are chosen randomly from 4 fixed locations in the grid, each assigned a different letter.  The starting location of the taxicab is also chosen randomly.\n",
    "\n",
    "The agent has 6 actions: 4 for movement, 1 for pickup, and 1 for drop-off.  Attempting a pickup when there is no passenger at the location incurs a reward of -10.  Dropping off a passenger outside one of the four designated zones is prohibited, and attempting it also incurs a reward of −10.  Dropping the passenger off at the correct destination provides the agent with a reward of 20.  Otherwise, the agent incurs a reward of −1 per time step.\n",
    "\n",
    "Your job is to train your agent until it converges to the optimal state-action value function.  You will have to think carefully about algorithm implementation, especially exploration parameters.\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a fundamental reinforcement-learning algorithm that has been successfully used to solve a variety of decision-making problems.   Like  Sarsa,  it is a model-free method based on temporal-difference learning. However, unlike Sarsa, Q-learning is *off-policy*, which means the policy it learns about can be different than the policy it uses to generate its behavior.  In Q-learning, this *target* policy is the greedy policy with respect to the current value-function estimate.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "- You should return the optimal *Q-value* for a specific state-action pair of the Taxi environment.\n",
    "\n",
    "- To solve this problem, you should implement the Q-learning algorithm and use it to solve the Taxi environment. The agent should explore the MDP, collect data to learn an optimal policy and also the optimal Q-value function.  Be mindful of how you handle terminal states: if $S_t$ is a terminal state, then $V(St)$ should always be 0.  Use $\\gamma= 0.90$ - this is important, as the optimal value function depends on the discount rate.  Also, note that an $\\epsilon$-greedy strategy can find an optimal policy despite finding sub-optimal Q-values.   As we are looking for optimal  Q-values, you will have to carefully consider your exploration strategy.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Lesson 7: Exploring Exploration\n",
    "\n",
    "-   Chapter 6 (6.5 Q-learning: Off-policy TD Control) of http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "-   Chapter 2 (2.6.1 Q-learning) of 'Algorithms for Sequential Decision Making', M.\n",
    "    Littman, 1996\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Syllabus page for this assignment.\n",
    "\n",
    "-   Use the template code to implement your work.\n",
    "\n",
    "-   Please use *python 3.6.x*, *gym==0.17.2*, *numpy==1.18.0* or their\n",
    "    more recent versions, and you can use any core library (i.e., anything\n",
    "    in the Python standard library). No other library can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# gym==0.17.2\n",
    "# numpy==1.18.0\n",
    "# Xingyan Liu\n",
    "# October 16, 2024\n",
    "################\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_min=0.1, epsilon_decay=0.995, episodes=2000000):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent with given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - alpha: Learning rate, controls how much new information overrides old information.\n",
    "        - gamma: Discount factor, determines the importance of future rewards.\n",
    "        - epsilon: Initial exploration rate for epsilon-greedy policy.\n",
    "        - epsilon_min: Minimum epsilon value, defines the lowest exploration rate allowed.\n",
    "        - epsilon_decay: Factor to reduce epsilon after each episode.\n",
    "        - episodes: Number of training episodes.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.episodes = episodes\n",
    "        self.env = gym.make(\"Taxi-v3\").env  # Initialize the Taxi-v3 environment\n",
    "        self.Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])  # Initialize Q-table to zeros\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - action: The chosen action, either exploratory (random) or exploitative (greedy).\n",
    "        \"\"\"\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore: choose a random action\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])  # Exploit: choose action with the highest Q-value\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the Q-learning algorithm over multiple episodes.\n",
    "        Updates the Q-table based on the agent's experiences.\n",
    "        \"\"\"\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()  # Reset the environment for a new episode\n",
    "            done = False  # Variable to track if the episode has finished\n",
    "\n",
    "            while not done:\n",
    "                # Select action based on epsilon-greedy policy\n",
    "                action = self.epsilon_greedy_policy(state)\n",
    "                \n",
    "                # Perform the action and observe the next state, reward, and done flag\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Q-learning update rule\n",
    "                self.Q[state][action] += self.alpha * (\n",
    "                    reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state][action]\n",
    "                )\n",
    "\n",
    "                state = next_state  # Move to the next state\n",
    "\n",
    "            # Decay epsilon after each episode to reduce exploration over time\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Solve the environment by training the agent using the Q-learning algorithm.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "    def Q_table(self, state, action):\n",
    "        \"\"\"\n",
    "        Get the Q-value for a specific state-action pair from the Q-table.\n",
    "        \n",
    "        Parameters:\n",
    "        - state: The current state.\n",
    "        - action: The chosen action.\n",
    "\n",
    "        Returns:\n",
    "        - Q-value: The value of the state-action pair.\n",
    "        \"\"\"\n",
    "        return self.Q[state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_case_1 (__main__.TestQNotebook.test_case_1) ... ok\n",
      "test_case_2 (__main__.TestQNotebook.test_case_2) ... ok\n",
      "test_case_3 (__main__.TestQNotebook.test_case_3) ... ok\n",
      "test_case_4 (__main__.TestQNotebook.test_case_4) ... ok\n",
      "test_case_5 (__main__.TestQNotebook.test_case_5) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 398.612s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x24de85a8150>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.agent = QLearningAgent()\n",
    "        cls.agent.solve()\n",
    "        \n",
    "    def test_case_1(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(462, 4),\n",
    "            -11.374402515,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_2(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(398, 3),\n",
    "            4.348907,\n",
    "            decimal=3\n",
    "        )\n",
    "    \n",
    "    def test_case_3(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(253, 0),\n",
    "            -0.5856821173,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_4(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(377, 1),\n",
    "            9.683,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_5(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(83, 5),\n",
    "            -13.9968,\n",
    "#            -12.8232,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
