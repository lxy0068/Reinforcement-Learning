{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #3\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "For this assignment,  you will build a Sarsa agent which will learn policies in the [OpenAI Gym](http://gym.openai.com/docs/) Frozen Lake environment.  [OpenAI Gym](http://gym.openai.com/docs/) is a platform where users can test their RL algorithms on a selection of carefully crafted environments.  As we will continue to use [OpenAI Gym](http://gym.openai.com/docs/) through Project 2, this assignment also provides an opportunity to familiarize yourself with its interface.\n",
    "\n",
    "Frozen Lake is a grid world environment that is highly stochastic,  where the agent must cross a slippery frozen  lake  which  has  deadly  holes  to  fall  through.   The  agent  begins  in  the  starting  state `S` and  is  given  a reward of `1` if it reaches the goal state `G`.  A reward of `0` is given for all other transitions.\n",
    "\n",
    "The agent can take one of four possible moves at each state (left, down, right, or up).  The frozen cells `F` are slippery, so the agent’s actions succeed only `1/3` of the time, while the other `2/3` are split evenly between the two directions orthogonal to the intended direction.  If the agent lands in a hole `H`, then the episode terminates. You will be given a randomized Frozen Lake map with a corresponding set of parameters to train your Sarsa agent with.\n",
    "\n",
    "\n",
    "$$\\text{Sarsa } (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$$\n",
    "\n",
    "Sarsa uses temporal-difference learning to form a model-free on-policy reinforcement-learning algorithm that solves the *control* problem. It is model free because it does not need and does not use a model of the environment, namely neither a transition nor reward function; instead, Sarsa samples transitions and rewards online.\n",
    "\n",
    "It is on-policy because it learns about the same policy that generates its behaviors (this is in contrast to *Q-learning*).  That is, Sarsa estimates the action-value function of its behavior policy.  In this homework,  you will not be training a Sarsa agent to approximate the *optimal* action-value function; instead, the hyperparameters of both the exploration strategy and the algorithm will be given to you as input — the goal being to verify that your SARSA agent is correctly implemented.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "Attention to detail to each of the following points is required:\n",
    "\n",
    "- You must use Python and the library NumPy for this homework *python 3.6.x* and *numpy==1.18.0*, or more recent version.\n",
    "\n",
    "- Install OpenAI Gym (e.g.pip install gym) *gym==0.17.2*\n",
    "\n",
    "- The Frozen Lake environment has been instantiated for you.\n",
    "- The pertinent random number generators have been seeded for you. Do *not* use the Python standard library’s *random* library.\n",
    "\n",
    "- Implement your Sarsa agent using an $\\epsilon$-greedy behavioral policy.  Specifically, you must use *numpy.random.random* to  choose  whether  or  not  the  action  is  greedy,  and *numpy.random.randint* to select the random action.\n",
    "\n",
    "- Initialize the agent’s Q-table to zeros.\n",
    "\n",
    "- Train your agent using the given input parameters.  The input *amap* is the Frozen Lake map that you need to resize and provide to the *desc* attribute when you instantiate your environment.  The input *gamma* is the discount rate.  The input *alpha* is the learning rate.  The input *epsilon* is the parameter for the $\\epsilon$-greedy behavior strategy your Sarsa agent will use.  Specifically, an action should be selected uniformly at random if a random number drawn uniformly between 0 and 1 is less than $\\epsilon$.  If the greedy action is selected,  the  action  with  lowest  index  should  be  selected  in  case  of  ties.   The  input `n_episodes` is  the number of episodes to train your agent.  Finally, *seed* is the number used to seed both Gym’s random number generator and NumPy’s random number generator.\n",
    "\n",
    "- Your Sarsa implementation should select the action corresponding to the next state the  agent  will  visit *even when* that  next  state  is  a  terminal  state.\n",
    "\n",
    "- You should return the greedy policy with respect to the Q-function obtained by your Sarsa agent after the completion of the final episode.  Specifically, the policy should be expressed as a string ofcharacters: **<, v, >, ^,** representing left, down, right, and up, respectively.  The ordering of the actions in the output should reflect the ordering of states in *amap*. \n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Chapter 6 (6.4 Sarsa:  On-policy TD Control) of\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Syllabus page for this assignment.\n",
    "-   \tUse the template code to implement your wor.\n",
    "-   \tPlease use python 3.6.x or more recent version, gym==0.17.2 and numpy==1.18.0 or more recent version, and you can use any core library  (i.e., anything in the Python standard library). No other library can be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# numpy==1.18.0\n",
    "# gym==0.17.2\n",
    "# 刘兴琰\n",
    "# 202264690069\n",
    "################\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from gym.envs import toy_text\n",
    "\n",
    "class FrozenLakeAgent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def amap_to_gym(self, amap='FFGG'):\n",
    "        \"\"\"\n",
    "        Converts the input map (amap) string into a Gym environment.\n",
    "        \n",
    "        Parameters:\n",
    "        - amap: A string representing the Frozen Lake map (e.g., 'SFFFHFFF...').\n",
    "               'S' - Start state, 'F' - Frozen (safe), 'H' - Hole (fall), 'G' - Goal.\n",
    "               \n",
    "        Returns:\n",
    "        - A Frozen Lake environment instantiated with the custom map provided by amap.\n",
    "        \"\"\"\n",
    "        amap = np.asarray(amap, dtype='c')   \n",
    "        side = int(sqrt(amap.shape[0]))   \n",
    "        amap = amap.reshape((side, side))   \n",
    "        return gym.make('FrozenLake-v0', desc=amap).unwrapped   \n",
    "    def epsilon_greedy(self, Q, state, epsilon, n_actions):\n",
    "        \"\"\"\n",
    "        Selects an action using the epsilon-greedy policy.\n",
    "        \n",
    "        Parameters:\n",
    "        - Q: The Q-table (state-action value table).\n",
    "        - state: The current state of the agent.\n",
    "        - epsilon: Probability of selecting a random action for exploration (controls exploration vs exploitation).\n",
    "        - n_actions: Total number of actions available in the environment.\n",
    "        \n",
    "        Returns:\n",
    "        - The action chosen based on the epsilon-greedy strategy. It chooses a random action with probability epsilon,\n",
    "          otherwise it selects the action with the highest Q-value for the current state (exploitation).\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(n_actions)   \n",
    "        else:\n",
    "            return np.argmax(Q[state])   \n",
    "\n",
    "    def solve(self, amap, gamma, alpha, epsilon, n_episodes, seed):\n",
    "        \"\"\"\n",
    "        Train the SARSA agent using the provided parameters and return the learned greedy policy.\n",
    "        \n",
    "        Parameters:\n",
    "        - amap: The map (string) for Frozen Lake (as explained above).\n",
    "        - gamma: Discount factor (determines the weight of future rewards, closer to 1 values future rewards higher).\n",
    "        - alpha: Learning rate (determines the extent to which newly acquired information overrides old information).\n",
    "        - epsilon: Epsilon for epsilon-greedy policy (higher value means more exploration).\n",
    "        - n_episodes: Number of episodes to run for training (each episode is one full run of the environment).\n",
    "        - seed: Random seed for reproducibility of the experiment.\n",
    "        \n",
    "        Returns:\n",
    "        - A policy string of characters representing the greedy action for each state ('<', 'v', '>', '^').\n",
    "          The actions map to left, down, right, and up, respectively.\n",
    "        \"\"\"\n",
    "        env = self.amap_to_gym(amap)\n",
    "        np.random.seed(seed)  \n",
    "        env.seed(seed)  \n",
    "        \n",
    "        n_actions = env.action_space.n  \n",
    "        n_states = env.observation_space.n  \n",
    "\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "\n",
    " \n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()   \n",
    "            action = self.epsilon_greedy(Q, state, epsilon, n_actions)   \n",
    "            \n",
    "            done = False   \n",
    "            while not done:\n",
    "                next_state, reward, done, _ = env.step(action)   \n",
    "                next_action = self.epsilon_greedy(Q, next_state, epsilon, n_actions)   \n",
    "                \n",
    "  \n",
    "                Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "\n",
    " \n",
    "                state, action = next_state, next_action\n",
    "        \n",
    " \n",
    "        policy = ''\n",
    "        actions_map = ['<', 'v', '>', '^']   \n",
    "        for state in range(n_states):\n",
    "            best_action = np.argmax(Q[state])   \n",
    "            policy += actions_map[best_action]   \n",
    "        \n",
    "        return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_case_1 (__main__.TestQNotebook.test_case_1) ... ok\n",
      "test_case_2 (__main__.TestQNotebook.test_case_2) ... ok\n",
      "test_case_3 (__main__.TestQNotebook.test_case_3) ... ok\n",
      "test_case_4 (__main__.TestQNotebook.test_case_4) ... ok\n",
      "test_case_5 (__main__.TestQNotebook.test_case_5) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 26.279s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1aca000cb10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.agent = FrozenLakeAgent()\n",
    "\n",
    "    def test_case_1(self):\n",
    "        example1 = self.agent.solve(\n",
    "            amap='SFFFHFFFFFFFFFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.25,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=14697,\n",
    "            seed=741684\n",
    "        )\n",
    "        assert(example1 == '^vv><>>vvv>v>>><')\n",
    "\n",
    "    def test_case_2(self):\n",
    "        example2 = self.agent.solve(\n",
    "            amap='SFFFFHFFFFFFFFFFFFFFFFFFG',\n",
    "            gamma=0.91,\n",
    "            alpha=0.12,\n",
    "            epsilon=0.13,\n",
    "            n_episodes=42271,\n",
    "            seed=983459\n",
    "        )\n",
    "        assert(example2 == '^>>>><>>>vvv>>vv>>>>v>>^<')\n",
    "\n",
    "    def test_case_3(self):\n",
    "        example3 = self.agent.solve(\n",
    "            amap='SFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.24,\n",
    "            epsilon=0.09,\n",
    "            n_episodes=49553,\n",
    "            seed=20240\n",
    "        )\n",
    "        assert(example3 == '<<v<')\n",
    "\n",
    "    def test_case_4(self):\n",
    "        example4 = self.agent.solve(\n",
    "            amap='SFFHHFFHHFFHHFFG',\n",
    "            gamma=0.99,\n",
    "            alpha=0.5,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=23111,\n",
    "            seed=44323\n",
    "        )\n",
    "        assert(example4=='^><<<>^<<><<<>^<')\n",
    "\n",
    "    def test_case_5(self):\n",
    "        example5 = self.agent.solve(\n",
    "            amap='SFFFFHFFFHHFFFFFFFFHHFFFG',\n",
    "            gamma=0.88,\n",
    "            alpha=0.15,\n",
    "            epsilon=0.16,\n",
    "            n_episodes=112312,\n",
    "            seed=6854343\n",
    "        )\n",
    "        assert(example5 == '^>><^<>><<<>v<^v>v<<<>vv<')\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
